{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f5984",
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1631624278412,
     "user": {
      "displayName": "Goodhope Kudakwashe Dhliwayo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsRy_XVOaYsCQOMapYlqXb9HUufz2YO1znKekNzA=s64",
      "userId": "14372767889095656787"
     },
     "user_tz": -60
    },
    "id": "ef554ba4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, roc_curve, auc, classification_report, multilabel_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from imutils import paths\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ad2f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1631624278413,
     "user": {
      "displayName": "Goodhope Kudakwashe Dhliwayo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsRy_XVOaYsCQOMapYlqXb9HUufz2YO1znKekNzA=s64",
      "userId": "14372767889095656787"
     },
     "user_tz": -60
    },
    "id": "SchYM_JSY86y"
   },
   "outputs": [],
   "source": [
    "run_env = 'local' # local, colab\n",
    "ds_type = 'raw_images' # np_tensor_array, raw_images\n",
    "datagen_api = 'keras' # keras, tf\n",
    "model_to_load = 'last' # last, best_val_loss\n",
    "\n",
    "method_name = 'EfficientNet-b7' # Model 1, ResNet50, EfficientNet-b7\n",
    "\n",
    "train_the_model = True\n",
    "predict_val_set = True\n",
    "predict_test_set = True\n",
    "save_tfjs_model = False\n",
    "\n",
    "train_size=0.7\n",
    "training_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242bd171",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1631624278414,
     "user": {
      "displayName": "Goodhope Kudakwashe Dhliwayo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsRy_XVOaYsCQOMapYlqXb9HUufz2YO1znKekNzA=s64",
      "userId": "14372767889095656787"
     },
     "user_tz": -60
    },
    "id": "74744268"
   },
   "outputs": [],
   "source": [
    "prediction_threshold = 0.7\n",
    "target_size = (224,224)\n",
    "batch_size = 64\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad9027",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1631624279221,
     "user": {
      "displayName": "Goodhope Kudakwashe Dhliwayo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsRy_XVOaYsCQOMapYlqXb9HUufz2YO1znKekNzA=s64",
      "userId": "14372767889095656787"
     },
     "user_tz": -60
    },
    "id": "7775ef1d",
    "outputId": "01a118bc-0863-4183-c044-203eb61e02b0"
   },
   "outputs": [],
   "source": [
    "if run_env == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    working_dir = '/content/drive/MyDrive/PFE'\n",
    "    input_dir = os.path.join(working_dir,'dataset')\n",
    "    !cp {working_dir}/scripts . -r\n",
    "else:\n",
    "    working_dir = os.getcwd()\n",
    "    input_dir = os.path.join(working_dir,'dataset')\n",
    "\n",
    "from scripts.ProgressBar import ProgressBar\n",
    "from scripts.TFImageDataGenerator import TFImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d14b05",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUhrGZDYTo2A"
   },
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "\n",
    "run_acc = 'cpu'\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name == '/device:GPU:0':\n",
    "    run_acc = 'gpu'\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    run_acc = 'tpu'\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c60ff1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a0f09b39"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists( working_dir+'/plots' ) : os.mkdir( working_dir + '/plots' )\n",
    "plots_dir = os.path.join( working_dir, 'plots', method_name )\n",
    "if not os.path.exists( plots_dir ) : os.mkdir( plots_dir )\n",
    "\n",
    "if not os.path.exists( working_dir+'/saved_actions' ) : os.mkdir( working_dir + '/saved_actions' )\n",
    "saved_actions_dir = os.path.join( working_dir, 'saved_actions', method_name )\n",
    "if not os.path.exists( saved_actions_dir ) :  os.mkdir( saved_actions_dir )\n",
    "\n",
    "tensorboard_callbacks_dir = os.path.join( saved_actions_dir, 'tensorboard_callbacks' )\n",
    "if not os.path.exists( tensorboard_callbacks_dir ) :  os.mkdir( tensorboard_callbacks_dir )\n",
    "\n",
    "pretrained_weights_dir = os.path.join( working_dir, 'pretrained_weights' )\n",
    "if not os.path.exists( pretrained_weights_dir ) :  os.mkdir( pretrained_weights_dir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01dfd1",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f34195fe"
   },
   "outputs": [],
   "source": [
    "if ds_type == 'raw_images':\n",
    "    train_set_input_file = pd.read_csv(input_dir+'/train_set.csv',converters={\"label_names\": literal_eval})\n",
    "    _train_set_rel_paths = train_set_input_file['rel_paths']\n",
    "    _train_set_label_names = train_set_input_file['label_names']\n",
    "    train_set_dir = os.path.join( input_dir, 'train_set' )\n",
    "else:\n",
    "    train_set_input_object = np.load(input_dir+'/train_set.npz',allow_pickle=True)\n",
    "    train_set_image_tensors = train_set_input_object['image_tensors']\n",
    "    train_set_label_names = train_set_input_object['label_names']\n",
    "\n",
    "test_set_input_file = pd.read_csv(input_dir+'/test_set.csv',converters={\"label_names\": literal_eval})\n",
    "test_set_rel_paths = test_set_input_file['rel_paths']\n",
    "test_set_label_names = test_set_input_file['label_names']\n",
    "\n",
    "test_set_dir = os.path.join( input_dir, 'test_set' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430aed4",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1b9713dd"
   },
   "outputs": [],
   "source": [
    "if ds_type == 'raw_images':\n",
    "    (train_set_rel_paths, val_set_rel_paths, train_set_label_names, val_set_label_names, train_set_indices, val_set_indices) = train_test_split( _train_set_rel_paths, _train_set_label_names, np.arange(len(_train_set_rel_paths)), train_size=train_size, random_state=random_state)\n",
    "else:\n",
    "    (train_set_image_tensor_ids, val_set_image_tensor_ids, train_set_label_names, val_set_label_names) = train_test_split( np.arange( len(train_set_image_tensors) ), train_set_label_names, train_size=train_size, random_state=random_state)\n",
    "    val_set_image_tensors = train_set_image_tensors[val_set_image_tensor_ids]\n",
    "    train_set_image_tensors = train_set_image_tensors[train_set_image_tensor_ids]\n",
    "\n",
    "train_set_label_binarizer = MultiLabelBinarizer()\n",
    "val_set_label_binarizer = MultiLabelBinarizer()\n",
    "test_set_label_binarizer = MultiLabelBinarizer()\n",
    "\n",
    "train_set_labels = train_set_label_binarizer.fit_transform(train_set_label_names)\n",
    "val_set_labels = val_set_label_binarizer.fit_transform(val_set_label_names)\n",
    "test_set_labels = test_set_label_binarizer.fit_transform(test_set_label_names)\n",
    "\n",
    "dataset_classes = train_set_label_binarizer.classes_\n",
    "testset_classes = test_set_label_binarizer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab05ea9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9e084f7a"
   },
   "outputs": [],
   "source": [
    "train_set_data_class_count = [0] * (len(dataset_classes)+1)\n",
    "val_set_data_class_count = [0] * (len(dataset_classes)+1)\n",
    "test_set_data_class_count = [0] * (len(dataset_classes)+1)\n",
    "total_dataset_data_class_count = [0] * (len(dataset_classes)+1)\n",
    "\n",
    "for train_set_data_index in range( len(train_set_labels) ):\n",
    "    train_set_data_class_count[len(dataset_classes)] += 1\n",
    "    total_dataset_data_class_count[len(dataset_classes)] += 1\n",
    "    for train_set_label_index in range( len(dataset_classes) ) :\n",
    "        if ( train_set_labels[train_set_data_index][train_set_label_index] == 1 ):          \n",
    "            train_set_data_class_count[train_set_label_index] += 1\n",
    "            total_dataset_data_class_count[train_set_label_index] += 1         \n",
    "\n",
    "for val_set_data_index in range( len(val_set_labels) ):\n",
    "    val_set_data_class_count[len(dataset_classes)] += 1\n",
    "    total_dataset_data_class_count[len(dataset_classes)] += 1\n",
    "    for val_set_label_index in range( len(dataset_classes) ) :\n",
    "        if ( val_set_labels[val_set_data_index][val_set_label_index] == 1 ):          \n",
    "            val_set_data_class_count[val_set_label_index] += 1\n",
    "            total_dataset_data_class_count[val_set_label_index] += 1\n",
    "\n",
    "for test_set_data_index in range( len(test_set_labels) ):\n",
    "    test_set_data_class_count[len(dataset_classes)] += 1\n",
    "    total_dataset_data_class_count[len(dataset_classes)] += 1\n",
    "    for test_set_label_index in range( len(dataset_classes) ) :\n",
    "        for test_set_label_test_index in range( len(testset_classes) ) :\n",
    "            if ( test_set_labels[test_set_data_index][test_set_label_test_index] == 1 and dataset_classes[test_set_label_index] == testset_classes[test_set_label_test_index] ):          \n",
    "                test_set_data_class_count[test_set_label_index] += 1\n",
    "                total_dataset_data_class_count[test_set_label_index] += 1\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "data = [\n",
    "    train_set_data_class_count,\n",
    "    val_set_data_class_count,\n",
    "    test_set_data_class_count,\n",
    "    total_dataset_data_class_count\n",
    "]\n",
    "row_labels = ['Training images','Validation images', 'Test images', 'Total Images']\n",
    "column_labels = dataset_classes.tolist() + [ 'All Classes' ]\n",
    "\n",
    "df=pd.DataFrame(data,columns=column_labels)\n",
    "ax.set_title('Wheat Disease Identification Dataset Summary',fontweight ='bold')\n",
    "ax.axis('off')\n",
    "dataset_summary_table = ax.table(cellText=df.values,colLabels=df.columns,rowLabels=row_labels,loc='center')\n",
    "dataset_summary_table.scale(3, 3)\n",
    "plt.savefig(plots_dir+'/dataset_summary.png',bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed9126",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6a2a8bd3"
   },
   "outputs": [],
   "source": [
    "if ds_type == 'raw_images':\n",
    "    train_set_df = {\n",
    "        'abs_paths': [],\n",
    "        'label_names': []\n",
    "    }\n",
    "    for train_set_index in train_set_indices:\n",
    "        train_set_df['abs_paths'].append(os.path.abspath(train_set_dir+train_set_rel_paths[train_set_index]))\n",
    "        train_set_df['label_names'].append(train_set_label_names[train_set_index])\n",
    "\n",
    "    train_set_df = pd.DataFrame(train_set_df)\n",
    "\n",
    "    val_set_df = {\n",
    "        'abs_paths': [],\n",
    "        'label_names': []\n",
    "    }\n",
    "    for val_set_index in val_set_indices:\n",
    "        val_set_df['abs_paths'].append(os.path.abspath(train_set_dir+val_set_rel_paths[val_set_index]))\n",
    "        val_set_df['label_names'].append(val_set_label_names[val_set_index])\n",
    "\n",
    "    val_set_df = pd.DataFrame(val_set_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd79f1",
   "metadata": {
    "id": "38aa6553"
   },
   "source": [
    "test_set_df = {\n",
    "    'abs_paths': [],\n",
    "    'label_names': []\n",
    "}\n",
    "for test_set_index in range(len(test_set_labels)):\n",
    "    test_set_df['abs_paths'].append(os.path.abspath(test_set_dir+test_set_rel_paths[test_set_index]))\n",
    "    test_set_df['label_names'].append(test_set_label_names[test_set_index])\n",
    "\n",
    "test_set_df = pd.DataFrame(test_set_df)\n",
    "    \n",
    "val_set_labels = test_set_labels\n",
    "val_set_label_names = test_set_label_names\n",
    "val_set_rel_paths = test_set_rel_paths\n",
    "val_set_df = test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dc3cd",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "899cfd41"
   },
   "outputs": [],
   "source": [
    "preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "if method_name == 'ResNet50':\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
    "\n",
    "if method_name == 'ResNet50V2':\n",
    "    preprocessing_function=tf.keras.applications.resnet_v2.preprocess_input\n",
    "\n",
    "elif method_name == 'InceptionV3':\n",
    "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "elif method_name.split('-')[0] == 'EfficientNet':\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input # no longer does anything\n",
    "    \n",
    "elif method_name == 'MobileNet':\n",
    "    preprocessing_function=tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "elif method_name == 'MobileNetV2':\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    \n",
    "elif method_name == 'VGG16':\n",
    "    preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    "    \n",
    "elif method_name == 'VGG19':\n",
    "    preprocessing_function=tf.keras.applications.vgg19.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937a7c5",
   "metadata": {
    "id": "YPpN0QPLhZ0a"
   },
   "outputs": [],
   "source": [
    "if datagen_api == 'keras':\n",
    "    ImageDataGenerator = tf.keras.preprocessing.image.ImageDataGenerator\n",
    "else:\n",
    "    ImageDataGenerator = TFImageDataGenerator\n",
    "    \n",
    "augmented_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function,\n",
    "    rotation_range=30,\n",
    "    shear_range=15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8,1.2]\n",
    ")\n",
    "\n",
    "flat_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function,\n",
    ")\n",
    "\n",
    "if ds_type == 'raw_images':\n",
    "    train_set_datagen = augmented_datagen.flow_from_dataframe(\n",
    "        dataframe=train_set_df,\n",
    "        target_size=target_size,\n",
    "        x_col=\"abs_paths\",\n",
    "        y_col=\"label_names\",\n",
    "        batch_size=batch_size,\n",
    "        interpolation='lanczos',\n",
    "    )\n",
    "    val_set_datagen = flat_datagen.flow_from_dataframe(\n",
    "        dataframe=val_set_df,\n",
    "        target_size=target_size,\n",
    "        x_col=\"abs_paths\",\n",
    "        y_col=\"label_names\",\n",
    "        batch_size=batch_size,\n",
    "        interpolation='lanczos',\n",
    "        shuffle=False\n",
    "    )\n",
    "else:\n",
    "    train_set_datagen = augmented_datagen.flow(train_set_image_tensors, train_set_labels, batch_size=batch_size)\n",
    "    val_set_datagen = flat_datagen.flow(val_set_image_tensors, val_set_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129d4d1",
   "metadata": {
    "id": "jPw1X5oqHGdH"
   },
   "outputs": [],
   "source": [
    "if method_name == 'Model 1':\n",
    "    with strategy.scope():\n",
    "        # Sequential Model\n",
    "        model = tf.keras.models.Sequential()\n",
    "        # 1st layer\n",
    "        model.add(tf.keras.layers.Conv2D(16,(3,3),input_shape = (*target_size,3),padding='same',activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "        # 2nd Layer\n",
    "        model.add(tf.keras.layers.Conv2D(32,(3,3),padding='same',activation='relu'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(2,2))\n",
    "        # Flatten Layer\n",
    "        model.add(tf.keras.layers.Flatten())\n",
    "        # 1st Hidden Layer\n",
    "        model.add(tf.keras.layers.Dense(512,activation='relu',))\n",
    "        model.add(tf.keras.layers.Dropout(0.4))\n",
    "        # 2nd Hidden Layer\n",
    "        model.add(tf.keras.layers.Dense(256,activation='relu'))\n",
    "        # Output Layer\n",
    "        model.add(tf.keras.layers.Dense(len(dataset_classes),activation='sigmoid'))\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318ed1b",
   "metadata": {
    "id": "21c3f28a"
   },
   "outputs": [],
   "source": [
    "if method_name == 'ResNet50':\n",
    "    with strategy.scope():\n",
    "        base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=tf.keras.layers.Input(shape=(*target_size, 3)))\n",
    "        # num_layers = 175\n",
    "        # setting all except last 44 layers as untrainable\n",
    "        for layer in base_model.layers[:-44]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        #taking all except last 15 layers\n",
    "        model_outputs = base_model.layers[-15].output\n",
    "        model_outputs = tf.keras.layers.Flatten()(model_outputs)\n",
    "        model_outputs = tf.keras.layers.Dropout(0.8)(model_outputs)\n",
    "        model_outputs = tf.keras.layers.Dense(len(dataset_classes), activation='sigmoid')(model_outputs)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=base_model.input, outputs=model_outputs)\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "        model.compile( loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135e4fc",
   "metadata": {
    "id": "naVHQf_jC3Gn"
   },
   "outputs": [],
   "source": [
    "if method_name.split('-')[0] == 'EfficientNet':\n",
    "    version = method_name.split('-')[1]\n",
    "    if not os.path.exists( pretrained_weights_dir+'/noisy-student-'+version+'.h5' ):\n",
    "        !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/noisystudent/noisy_student_efficientnet-{version}.tar.gz\n",
    "        !tar -xf noisy_student_efficientnet-{version}.tar.gz\n",
    "        os.remove('noisy_student_efficientnet-'+version+'.tar.gz')\n",
    "        if not os.path.exists( 'scripts/efficientnet_weight_update_util.py' ):\n",
    "            !wget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/python/keras/applications/efficientnet_weight_update_util.py -O {working_dir}/scripts/efficientnet_weight_update_util.py\n",
    "            if run_env == 'colab':\n",
    "                !cp {working_dir}/scripts . -r\n",
    "        if version == 'b0':\n",
    "            !python scripts/efficientnet_weight_update_util.py --model b0 --notop --ckpt \\\n",
    "                noisy_student_efficientnet-b0/model.ckpt --o {pretrained_weights_dir}/noisy-student-b0.h5\n",
    "            shutil.rmtree('noisy_student_efficientnet-b0')\n",
    "        else:\n",
    "            !python scripts/efficientnet_weight_update_util.py --model {version} --notop --ckpt \\\n",
    "                noisy-student-efficientnet-{version}/model.ckpt --o {pretrained_weights_dir}/noisy-student-{version}.h5\n",
    "            shutil.rmtree('noisy-student-efficientnet-'+version)\n",
    "\n",
    "    with strategy.scope():\n",
    "        if version == 'b0':\n",
    "            bm = tf.keras.applications.EfficientNetB0\n",
    "        elif version == 'b1':\n",
    "            bm = tf.keras.applications.EfficientNetB1\n",
    "        elif version == 'b2':\n",
    "            bm = tf.keras.applications.EfficientNetB2\n",
    "        elif version == 'b3':\n",
    "            bm = tf.keras.applications.EfficientNetB3\n",
    "        elif version == 'b4':\n",
    "            bm = tf.keras.applications.EfficientNetB4\n",
    "        elif version == 'b5':\n",
    "            bm = tf.keras.applications.EfficientNetB5\n",
    "        elif version == 'b6':\n",
    "            bm = tf.keras.applications.EfficientNetB6\n",
    "        elif version == 'b7':\n",
    "            bm = tf.keras.applications.EfficientNetB7\n",
    "    \n",
    "        base_model = bm( weights=pretrained_weights_dir+'/noisy-student-'+version+'.h5', include_top=False ,input_shape=(*target_size,3))\n",
    "\n",
    "        base_model.trainable = True\n",
    "\n",
    "        model = tf.keras.Sequential([\n",
    "            base_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(len(dataset_classes), activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3cf5b",
   "metadata": {
    "id": "1a427f12"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "tf.keras.utils.plot_model( model, show_shapes=True, show_layer_names=True, to_file=plots_dir+'/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123b608",
   "metadata": {
    "id": "adsgxgY6manw"
   },
   "outputs": [],
   "source": [
    "# Learning rate callback\n",
    "def learning_rate_adjust(epoch):\n",
    "    LR_START = 0.00001\n",
    "    LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "    LR_MIN = 0.00001\n",
    "    LR_RAMPUP_EPOCHS = 5\n",
    "    LR_SUSTAIN_EPOCHS = 0\n",
    "    LR_EXP_DECAY = .8\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "learning_rate_callback = tf.keras.callbacks.LearningRateScheduler(learning_rate_adjust, verbose=1)\n",
    "\n",
    "if method_name.split('-')[0] == 'EfficientNet':\n",
    "    rang = np.arange(training_epochs)\n",
    "    y = [learning_rate_adjust(x) for x in rang]\n",
    "    x_bounds = np.arange(1, training_epochs+1)\n",
    "    x_lim = [1, training_epochs]\n",
    "    if (training_epochs<=20): plt.xticks(np.arange(round(0,training_epochs), training_epochs, 2))\n",
    "    plt.plot(x_bounds, y)\n",
    "    plt.xlim(x_lim)\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.title(\"Learning rate per epoch\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(plots_dir+'/learning_rate_per_epoch.png', dpi=140)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# tensorboard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(tensorboard_callbacks_dir, histogram_freq=1)\n",
    "\n",
    "# checkpoint callback\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath= saved_actions_dir+'/model_best_val_loss',\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "csv_logger_callback = tf.keras.callbacks.CSVLogger(saved_actions_dir+'/model_log.csv', append=False, separator=',')\n",
    "\n",
    "callbacks = []\n",
    "if method_name.split('-')[0] == 'EfficientNet':\n",
    "    callbacks.append(learning_rate_callback)\n",
    "if not run_acc == 'tpu':\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    callbacks.append(model_checkpoint_callback)\n",
    "callbacks.append(early_stopping_callback)\n",
    "callbacks.append(csv_logger_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee090e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UdcKbS8rJtXk"
   },
   "outputs": [],
   "source": [
    "if train_the_model:\n",
    "    \n",
    "    def warn_bypass(*args, **kwargs):\n",
    "            pass\n",
    "    import warnings\n",
    "    warn = warnings.warn\n",
    "    warnings.warn = warn_bypass\n",
    "\n",
    "    eargs = {}\n",
    "    if ds_type == 'raw_images':\n",
    "        eargs['workers'] = 8\n",
    "\n",
    "    try:\n",
    "        model_fit_object = model.fit(\n",
    "            train_set_datagen,\n",
    "            validation_data=val_set_datagen,\n",
    "            validation_steps=len(val_set_labels) // batch_size,\n",
    "            steps_per_epoch=len(train_set_labels) // batch_size,\n",
    "            epochs=training_epochs,\n",
    "            callbacks = callbacks,\n",
    "            **eargs\n",
    "        )\n",
    "        \n",
    "        model_history = model_fit_object.history\n",
    "        np.save(saved_actions_dir+'/model_history.npy', model_history)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    model.save(saved_actions_dir+'/model.h5')\n",
    "    \n",
    "    warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ad629",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "6b18a984"
   },
   "outputs": [],
   "source": [
    "model_history = np.load(saved_actions_dir+'/model_history.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "##################\n",
    "\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "arrowprops=dict(arrowstyle=\"-\",connectionstyle=\"angle,angleA=0,angleB=120\")\n",
    "kw = dict(xycoords='data',textcoords=\"axes fraction\", arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"bottom\")\n",
    "\n",
    "max_accuracy = max(model_history['accuracy'])\n",
    "max_accuracy_epoch = model_history['accuracy'].index(max_accuracy) + 1\n",
    "max_accuracy_text= \"max={:.4f} at epoch {:.0f}\".format(max_accuracy, max_accuracy_epoch)\n",
    "\n",
    "max_val_accuracy = max(model_history['val_accuracy'])\n",
    "max_val_accuracy_epoch = model_history['val_accuracy'].index(max_val_accuracy) + 1\n",
    "max_val_accuracy_text= \"max={:.4f} at epoch {:.0f}\".format(max_val_accuracy, max_val_accuracy_epoch)\n",
    "\n",
    "min_loss = min(model_history['loss'])\n",
    "min_loss_epoch = model_history['loss'].index(min_loss) + 1\n",
    "min_los_text= \"min={:.4f} at epoch {:.0f}\".format(min_loss, min_loss_epoch)\n",
    "\n",
    "min_val_loss = min(model_history['val_loss'])\n",
    "min_val_loss_epoch = model_history['val_loss'].index(min_val_loss) + 1\n",
    "min_val_loss_text= \"min={:.4f} at epoch {:.0f}\".format(min_val_loss, min_val_loss_epoch)\n",
    "\n",
    "##################\n",
    "\n",
    "N = len(model_history['accuracy'])\n",
    "x_bounds = np.arange(1, N+1)\n",
    "x_lim = [1, N]\n",
    "\n",
    "##################\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(x_bounds, model_history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(x_bounds, model_history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title(method_name+' Wheat Disease Identification Model Training vs Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "y=model_history['accuracy'] + model_history['val_accuracy']\n",
    "plt.yticks(np.arange(round(min(y),1), 1., 0.05))\n",
    "if N<=30: plt.xticks(np.arange(round(0,N), N, 2))\n",
    "plt.xlim(x_lim)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.annotate(max_accuracy_text, xy=(max_accuracy_epoch, max_accuracy), xytext=(0.7,max_accuracy+0.03), **kw)\n",
    "plt.annotate(max_val_accuracy_text, xy=(max_val_accuracy_epoch, max_val_accuracy), xytext=(0.95,max_val_accuracy-0.07), **kw)\n",
    "plt.savefig(plots_dir+'/accuracy.png', dpi=140)\n",
    "#plt.show()\n",
    "plt.clf()\n",
    "\n",
    "##################\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(x_bounds,model_history['loss'], label='Training Loss')\n",
    "plt.plot(x_bounds,model_history['val_loss'], label='Validation Loss')\n",
    "plt.title(method_name+' Wheat Disease Identification Model Training vs Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "y=model_history['loss'] + model_history['val_loss']\n",
    "factor = np.ceil(max(y))\n",
    "plt.yticks(np.arange(round(min(y),1), round(max(y),1)+0.1*factor, 0.05*factor))\n",
    "if N<=30: plt.xticks(np.arange(round(0,N), N, 2))\n",
    "plt.xlim(x_lim)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "plt.annotate(min_los_text, xy=(min_loss_epoch, min_loss), xytext=(0.95,min_loss+0.1), **kw)\n",
    "plt.annotate(min_val_loss_text, xy=(min_val_loss_epoch, min_val_loss), xytext=(0.7,min_val_loss+0.1), **kw)\n",
    "plt.savefig(plots_dir+'/loss.png', dpi=140)\n",
    "#plt.show()\n",
    "plt.clf()\n",
    "\n",
    "##################\n",
    "\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "##################\n",
    "\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(x_bounds, model_history['accuracy'], label='Training Accuracy')\n",
    "ax.plot(x_bounds, model_history['val_accuracy'], label='Validation Accuracy')\n",
    "ax.set_title(method_name+' Wheat Disease Identification Model Training vs Validation Accuracy')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "y=model_history['accuracy'] + model_history['val_accuracy']\n",
    "ax.set_yticks(np.arange(round(min(y),1), 1., 0.05))\n",
    "if N<=30: ax.set_xticks(np.arange(round(0,N), N, 2))\n",
    "ax.set_xlim(x_lim)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='lower right')\n",
    "ax.annotate(max_accuracy_text, xy=(max_accuracy_epoch, max_accuracy), xytext=(0.7,max_accuracy+0.03), **kw)\n",
    "ax.annotate(max_val_accuracy_text, xy=(max_val_accuracy_epoch, max_val_accuracy), xytext=(0.95,max_val_accuracy-0.07), **kw)\n",
    "\n",
    "##################\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.plot(x_bounds, model_history['loss'], label='Training Loss')\n",
    "ax2.plot(x_bounds, model_history['val_loss'], label='Validation Loss')\n",
    "ax2.set_title(method_name+' Wheat Disease Identification Model Training vs Validation Loss')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "y=model_history['loss'] + model_history['val_loss']\n",
    "factor = np.ceil(max(y))\n",
    "ax2.set_yticks(np.arange(round(min(y),1), round(max(y),1)+0.1*factor, 0.05*factor))\n",
    "if N<=30: ax2.set_xticks(np.arange(round(0,N), N, 2))\n",
    "ax2.set_xlim(x_lim)\n",
    "ax2.grid(True)\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.annotate(min_los_text, xy=(min_loss_epoch, min_loss), xytext=(0.95,min_loss+0.1), **kw)\n",
    "ax2.annotate(min_val_loss_text, xy=(min_val_loss_epoch, min_val_loss), xytext=(0.7,min_val_loss+0.1), **kw)\n",
    "\n",
    "##################\n",
    "\n",
    "#plt.savefig(plots_dir+'/accuracy_loss.png')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0991c740",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0f352202",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if save_tfjs_model or predict_val_set or predict_test_set:\n",
    "    if not model_to_load == 'last':\n",
    "        model = tf.keras.models.load_model(saved_actions_dir+'/model_'+model_to_load)\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(saved_actions_dir+'/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce771e0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a016b3f0"
   },
   "outputs": [],
   "source": [
    "if save_tfjs_model:\n",
    "    tfjs.converters.save_keras_model(model, saved_actions_dir+'/model_tfjs')\n",
    "    with open(saved_actions_dir+'/model_tfjs/dataset_classes.json', \"w\") as json_file:\n",
    "        json.dump(dataset_classes.tolist(), json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918ec20",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cdcc928b"
   },
   "outputs": [],
   "source": [
    "if predict_val_set:\n",
    "    val_set_pred_labels = model.predict(val_set_datagen, batch_size=batch_size)\n",
    "    np.save( saved_actions_dir+'/val_set_pred_labels', val_set_pred_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a8a70d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "e0230972"
   },
   "outputs": [],
   "source": [
    "val_set_pred_labels = np.load(saved_actions_dir+'/val_set_pred_labels.npy')\n",
    "\n",
    "val_set_pred_labels_normalized = []\n",
    "\n",
    "for prediction in val_set_pred_labels:\n",
    "    to_append = [1 if i>=prediction_threshold else 0 for i in prediction]\n",
    "    if not any(to_append):\n",
    "        new_threshold = max(prediction) - 0.1\n",
    "        if new_threshold > 0.1:\n",
    "            to_append = [1 if i>=new_threshold else 0 for i in prediction]\n",
    "        else:\n",
    "            to_append = [1 if i==max(prediction) else 0 for i in prediction]\n",
    "    val_set_pred_labels_normalized.append(to_append)\n",
    "val_set_pred_labels_normalized = np.array(val_set_pred_labels_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0ab5e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a8ef2eaf"
   },
   "outputs": [],
   "source": [
    "cr = classification_report(val_set_labels, val_set_pred_labels_normalized, target_names=dataset_classes, output_dict=True)\n",
    "plt.figure(figsize=(15, 7))\n",
    "ax = sns.heatmap(pd.DataFrame(cr).T, annot=True, annot_kws={\"size\": 12}, cmap=ListedColormap(['white']), linecolor='#efefef', linewidths=2, cbar=False, fmt='.4g')\n",
    "plt.title(method_name+' Classification Report',fontweight ='bold')\n",
    "plt.savefig(plots_dir+'/classification_report.png', dpi=140)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadd109",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8757c9ae"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(dataset_classes)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(val_set_labels[:, i], val_set_pred_labels[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(val_set_labels.ravel(), val_set_pred_labels.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(dataset_classes))]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(dataset_classes)):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= len(dataset_classes)\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]), color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]), color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "for i in range(len(dataset_classes)):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class \"{0}\" (area = {1:0.2f})'.format(dataset_classes[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.yticks(np.arange(0.1, 1.05, 0.1))\n",
    "plt.xticks(np.arange(0.1, 1., 0.1))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class: '+method_name,fontweight ='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(plots_dir+'/roc_curves.png', dpi=140)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757bbdb",
   "metadata": {
    "id": "5c4ab6e4"
   },
   "outputs": [],
   "source": [
    "# For each class\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(len(dataset_classes)):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(val_set_labels[:, i], val_set_pred_labels[:, i])\n",
    "    average_precision[i] = average_precision_score(val_set_labels[:, i], val_set_pred_labels[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(val_set_labels.ravel(),\n",
    "    val_set_pred_labels.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(val_set_labels, val_set_pred_labels, average=\"micro\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.grid(True)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(method_name+' Average precision score, micro-averaged over all classes: AP={0:0.4f}'.format(average_precision[\"micro\"]))\n",
    "\n",
    "plt.savefig(plots_dir+'/average-precision-score.png', dpi=140)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a18652",
   "metadata": {
    "id": "89c7270c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', linestyle=':')\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i in range(len(dataset_classes)):\n",
    "    l, = plt.plot(recall[i], precision[i])\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class \"{0}\" (area = {1:0.2f})'.format(dataset_classes[i], average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.3)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title(method_name+' Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.5), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.savefig(plots_dir+'/precision-recall_curves.png', dpi=140)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881e020",
   "metadata": {
    "id": "f46afc34",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = multilabel_confusion_matrix(val_set_labels, val_set_pred_labels_normalized)\n",
    "fig, axes = plt.subplots( round(len(cm)/2), 2, figsize=(20, 35))\n",
    "axes = axes.ravel()\n",
    "\n",
    "plt.title(method_name+' Wheat Disease Identification Confusion Matrix')\n",
    "for cm_index in range(len(cm)):\n",
    "    cmp = ConfusionMatrixDisplay(cm[cm_index],display_labels=['0','1'])\n",
    "    ax = plt.subplot( round( len(cm) / 2 ) , 2 , cm_index + 1 )\n",
    "    ax.set_title(dataset_classes[cm_index])\n",
    "    cmp.plot(ax=ax)\n",
    "\n",
    "plt.savefig(plots_dir+'/confusion_matrix.png', dpi=200)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a582eff",
   "metadata": {
    "id": "d2f59a3b"
   },
   "outputs": [],
   "source": [
    "val_set_pred_labels_max_indices = []\n",
    "val_set_labels_max_indices = []\n",
    "for item_index in range(len(val_set_labels)):\n",
    "    true_item = val_set_labels[item_index]\n",
    "    pred_item = val_set_pred_labels[item_index]\n",
    "    pred_item_normalized = val_set_pred_labels_normalized[item_index]\n",
    "    \n",
    "    pred_max_val = max(pred_item)\n",
    "    pred_max_index = pred_item.argmax()\n",
    "    \n",
    "    for i in range(len(dataset_classes)):\n",
    "        pred_index = pred_item.argmax()\n",
    "        if true_item[pred_index] == 1 and pred_item_normalized[pred_index] == 1:\n",
    "            val_set_pred_labels_max_indices.append(pred_index)\n",
    "            val_set_labels_max_indices.append(pred_index)\n",
    "            break\n",
    "        else:\n",
    "            if i == len(dataset_classes)-1:\n",
    "                val_set_pred_labels_max_indices.append(pred_max_index)\n",
    "                val_set_labels_max_indices.append(true_item.argmax())\n",
    "            else:\n",
    "                pred_item = np.delete(pred_item, pred_index)\n",
    "                \n",
    "cm = confusion_matrix(val_set_labels_max_indices, val_set_pred_labels_max_indices)\n",
    "cmp = ConfusionMatrixDisplay(cm, display_labels=dataset_classes)\n",
    "fig, ax = plt.subplots(figsize=(20,15))\n",
    "ax.set_title(method_name+' Wheat Disease Identification Confusion Matrix',fontweight ='bold')\n",
    "cmp.plot(ax=ax)\n",
    "    \n",
    "plt.savefig(plots_dir+'/confusion_matrix.png')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8550a",
   "metadata": {
    "id": "77685772",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if predict_test_set:\n",
    "    plt.subplots(figsize=(15, len(test_set_rel_paths)*3))\n",
    "\n",
    "    progress = ProgressBar( len(test_set_rel_paths), fmt=ProgressBar.FULL, init=\"Predicting\" )\n",
    "\n",
    "    for test_image_index in range(len(test_set_rel_paths)):\n",
    "\n",
    "        progress.current += 1\n",
    "        progress()\n",
    "\n",
    "        image = tf.keras.preprocessing.image.load_img( test_set_dir + test_set_rel_paths[test_image_index] )\n",
    "        width, height = image.size\n",
    "        image_resized = image.resize(target_size)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array( image_resized )\n",
    "        img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "        img_array = preprocessing_function(img_array)\n",
    "\n",
    "        prediction = model.predict( img_array )\n",
    "\n",
    "        result = \"\"\n",
    "        for prediction_class_index in range( len(prediction[0]) ):\n",
    "            result = result + dataset_classes[prediction_class_index] + \": \" + str(round(100*prediction[0][prediction_class_index],2)) + \"%\\n\"     \n",
    "\n",
    "        ax = plt.subplot( round( len(test_set_rel_paths) / 2 ) , 2 , test_image_index + 1 )\n",
    "\n",
    "        predicted = \"\"\n",
    "        predicted_class_indices = [i for i, x in enumerate(prediction[0]) if x >= prediction_threshold]\n",
    "        if len(predicted_class_indices) == 0:\n",
    "            new_threshold = max(prediction[0]) - 0.1\n",
    "            if new_threshold > 0.1:\n",
    "                predicted_class_indices = [i for i, x in enumerate(prediction[0]) if x >= new_threshold]\n",
    "            else:\n",
    "                predicted_class_indices = [prediction[0].argmax()]\n",
    "        cnt = 0\n",
    "        for predicted_class_index in predicted_class_indices:\n",
    "            predicted += dataset_classes[predicted_class_index]\n",
    "            if cnt < len(predicted_class_indices)-1: predicted += \", \"\n",
    "            cnt += 1\n",
    "        true = \"\"\n",
    "        true_class_indices = [i for i, x in enumerate(test_set_labels[test_image_index]) if x == 1]\n",
    "        cnt = 0\n",
    "        for true_class_index in true_class_indices:\n",
    "            true += testset_classes[true_class_index]\n",
    "            if cnt < len(true_class_indices)-1: true += \", \"\n",
    "            cnt += 1\n",
    "\n",
    "        ax.set_title( 'Predicted: ' + predicted )\n",
    "        ax.set_yticks([ height / 2 ])\n",
    "        ax.set_yticklabels([result])\n",
    "        ax.set_xticks([])\n",
    "\n",
    "        ax.set_xlabel( 'True: ' + true )\n",
    "\n",
    "        ax.imshow( image, extent=[0,width,0,height] )\n",
    "\n",
    "    progress.done()\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db936b5",
   "metadata": {
    "id": "e4aecc14"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wdi.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
